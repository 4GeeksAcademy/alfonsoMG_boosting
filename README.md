# Boosting Models Implementation Project in Machine Learning

In this project, I focused on implementing two different boosting models for machine learning applications: the XGBoost Classifier and the Gradient Boost Classifier. The dataset used in this project is the same managed in the previous project, enabling comparisons with results obtained from the decision tree and random forest models. While the dataset may not be optimal for training such models, the results obtained are valid for our main objective: learning and understanding these types of machine learning models.

## Project Contents

1. **Model Implementation:**
   - `XGBoost Classifier`: The implementation and evaluation of this boosting model are detailed, highlighting its features and results.
   - `Gradient Boost Classifier`: The implementation and evaluation of this model are described, emphasizing its characteristics and comparing it with other models.

2. **Results Comparison:**
   - A comparison of the results obtained with the boosting models is provided, in relation to the decision tree and random forest models.

3. **Used Dataset:**
   - The dataset managed in the previous project is utilized, allowing for a coherent and relevant comparison.

4. **Project Objectives:**
   - The primary focus of this project is on learning and understanding boosting models in machine learning, using valid results despite the limitations of the dataset.

## Usage Instructions

1. **Requirements:**
   - Ensure that the necessary libraries are installed, as specified in the `requirements.txt` file.

2. **Code Execution:**
   - Find the scripts for each model in the /src directory. Follow the detailed instructions in each folder to execute the corresponding code.

3. **Results and Analysis:**
   - Explore the obtained results and comparative analysis in the corresponding document.
   - The trained models are saved in the /models directory.

## Conclusions

This project provides a detailed insight into the implementation and evaluation of boosting models in machine learning. Despite the dataset not being ideal, the results obtained are valuable for educational purposes and understanding these types of models. The decision to use Gradient Boosting as the most effective model is based on minimizing overfitting, even though it may not present optimal conditions.

Thank you for exploring this project! Feel free to provide feedback, ask questions, or contribute to its development.