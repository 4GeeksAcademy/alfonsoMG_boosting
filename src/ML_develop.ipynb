{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import ParameterGrid, RandomizedSearchCV, GridSearchCV\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable specific FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Disable specific UserWarnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "# Ignore FitFailedWarning from scikit-learn\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.FitFailedWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"/workspaces/alfonsoMG_boosting/data/processed/diabetes_X_train.csv\")\n",
    "X_test = pd.read_csv(\"/workspaces/alfonsoMG_boosting/data/processed/diabetes_X_test.csv\")\n",
    "y_train = pd.read_csv(\"/workspaces/alfonsoMG_boosting/data/processed/diabetes_y_train.csv\")\n",
    "y_test = pd.read_csv(\"/workspaces/alfonsoMG_boosting/data/processed/diabetes_y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of training machine learning models in a GitHub Codespace with Python, the idea is to experiment with and evaluate two model boosting approaches, specifically XGBoost and Gradient Boost, both designed for addressing classification problems. The strategy involves conducting the training of both models and then performing a detailed analysis of the results to determine which one exhibits the most effective or favorable performance.\n",
    "\n",
    "The purpose of this comparison is to identify which of the two model boosting methods better suits the specific nature of the classification problem being addressed. By delving into this comparative evaluation, the goal is to understand the strengths and weaknesses of each approach to make informed decisions regarding the selection of the most suitable model for the given case. This evaluation process will provide a solid foundation for making implementation and parameter-tuning decisions that lead to optimal performance in the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **XGBoost Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for Train is 1.0.\n",
      "The accuracy score for Test is 0.7142857142857143.\n",
      "The accuracy difference between the models is 0.2857142857142857.\n"
     ]
    }
   ],
   "source": [
    "# Create an XGBoost classifier with a specified random state\n",
    "model_x = XGBClassifier(random_state=24)\n",
    "\n",
    "# Train the model on the training set\n",
    "model_x.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set and calculate the accuracy\n",
    "y_pred_train = model_x.predict(X_train)\n",
    "train_score = accuracy_score(y_train, y_pred_train)\n",
    "print(f'The accuracy score for Train is {train_score}.')\n",
    "\n",
    "# Make predictions on the testing set and calculate the accuracy\n",
    "y_pred_test = model_x.predict(X_test)\n",
    "test_score = accuracy_score(y_test, y_pred_test)\n",
    "print(f'The accuracy score for Test is {test_score}.')\n",
    "\n",
    "# Calculate and print the difference in accuracy between the training and testing sets\n",
    "difference = train_score - test_score\n",
    "print(f'The accuracy difference between the models is {difference}.')\n",
    "\n",
    "# Save the trained model to a file\n",
    "dump(model_x, open(\"/workspaces/alfonsoMG_boosting/models/x_boosting_default_model.pk\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gardient Boost Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for Train is 0.7947882736156352.\n",
      "The accuracy score for Test is 0.7727272727272727.\n",
      "The accuracy difference between the models is 0.022061000888362492.\n"
     ]
    }
   ],
   "source": [
    "# Create a Gradient Boosting classifier with specified parameters\n",
    "model = GradientBoostingClassifier(n_estimators=10, random_state=24)\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set and calculate the accuracy\n",
    "y_pred_train = model.predict(X_train)\n",
    "train_score = accuracy_score(y_train, y_pred_train)\n",
    "print(f'The accuracy score for Train is {train_score}.')\n",
    "\n",
    "# Make predictions on the testing set and calculate the accuracy\n",
    "y_pred_test = model.predict(X_test)\n",
    "test_score = accuracy_score(y_test, y_pred_test)\n",
    "print(f'The accuracy score for Test is {test_score}.')\n",
    "\n",
    "# Calculate and print the difference in accuracy between the training and testing sets\n",
    "difference = train_score - test_score\n",
    "print(f'The accuracy difference between the models is {difference}.')\n",
    "\n",
    "# Save the trained model to a file\n",
    "dump(model, open(\"/workspaces/alfonsoMG_boosting/models/gradient_boosting_default_model.pk\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the implementation of both classification models, our analysis revealed that XGBoost, while producing promising results on the training dataset, faced a substantial challenge of overfitting. In contrast, the Gradient Boosting option, although achieving a relatively lower accuracy, exhibited a minor issue with overfitting.\n",
    "\n",
    "Considering these findings, we have made the strategic decision to continue with the Gradient Boosting model for the next phase, which involves hyperparameter optimization. Our objective during this optimization process is twofold: to improve the overall accuracy of the model and to further minimize the gap between the performance on the training and test datasets.\n",
    "\n",
    "By choosing the Gradient Boosting model and focusing on hyperparameter tuning, we aim to strike a balance that maximizes accuracy while mitigating overfitting concerns. This approach aligns with our goal of achieving a more robust and generalizable model for the given classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gardient Boost Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for Train is 1.0.\n",
      "The accuracy score for Test is 0.7467532467532467.\n",
      "The resulting overfitting is 0.2532467532467533 points.\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grid for Gradient Boosting Classifier\n",
    "hyper = {\n",
    "    'n_estimators': [50, 100, 150],         # Number of trees in the ensemble\n",
    "    'learning_rate': [0.05, 0.1, 0.2],      # Learning rate (step of each tree)\n",
    "    'max_depth': [None] + [3, 10, 15],      # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 4, 6],         # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 5, 15],          # Minimum number of samples required to be a leaf\n",
    "    'subsample': [0.8, 1.0],                # Fraction of samples used to fit each tree\n",
    "    'max_features': [None, 'sqrt', 'log2'], \n",
    "    'random_state': [24]\n",
    "}\n",
    "\n",
    "# Create the Gradient Boosting Classifier model\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Perform hyperparameter tuning with cross-validation\n",
    "grid = GridSearchCV(estimator=gb_classifier, param_grid=hyper, cv=5)\n",
    "grid.fit(X_train, y_train)  \n",
    "best_hyper = grid.best_params_\n",
    "\n",
    "# Create the optimized model using the best hyperparameters\n",
    "model_opt = GradientBoostingClassifier(**best_hyper)\n",
    "model_opt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set and calculate the accuracy\n",
    "y_pred_train = model_opt.predict(X_train)\n",
    "train_score_opt = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"The accuracy score for Train is {train_score_opt}.\")\n",
    "\n",
    "# Make predictions on the testing set and calculate the accuracy\n",
    "y_pred_test = model_opt.predict(X_test)\n",
    "test_score_opt = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"The accuracy score for Test is {test_score_opt}.\")\n",
    "\n",
    "# Calculate and print the overfitting\n",
    "overfitting = train_score_opt - test_score_opt\n",
    "print(f\"The resulting overfitting is {overfitting} points.\")\n",
    "\n",
    "# Save the optimized model to a file\n",
    "dump(model_opt, open(\"/workspaces/alfonsoMG_boosting/models/gradient_boosting_opt_model.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **XGBoost Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delve deeper into the rationale and methodology behind choosing RandomSearch over GridSearch for the optimization and hyperparameter tuning of XGBoost.\n",
    "\n",
    "Hyperparameter tuning is a critical aspect of machine learning model development, influencing the model's performance and generalization to unseen data. GridSearch is a conventional method that exhaustively searches through a predefined set of hyperparameter values, evaluating the model's performance for each combination. While GridSearch is systematic, it can be computationally expensive, especially when dealing with a large search space.\n",
    "\n",
    "In contrast, RandomSearch takes a more randomized approach by sampling hyperparameter values from a specified distribution. This method offers several advantages. First, it allows exploration of a broader range of hyperparameter values, which may include those outside the scope of a grid search. This is particularly beneficial when the optimal values are not easily predictable or fall within a specific grid.\n",
    "\n",
    "Secondly, RandomSearch can be more computationally efficient. Since it randomly selects hyperparameter values, it has the potential to converge to an optimal solution more quickly, especially when dealing with a large search space. This can be advantageous in scenarios where computational resources are limited.\n",
    "\n",
    "By opting for RandomSearch, we aim to strike a balance between thorough exploration of hyperparameter space and computational efficiency. We will define a wide range for each hyperparameter, allowing for a comprehensive search. The iteration process will involve randomly selecting combinations of hyperparameter values, training the XGBoost model, and evaluating its performance. The best-performing configuration will be retained as the final model, providing an efficient and effective means of hyperparameter tuning for XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Best hyperparameters: {'subsample': 0.7000000000000001, 'random_state': 24, 'objective': 'binary:logistic', 'n_jobs': -1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 22, 'lambda': 9, 'grow_policy': 'depthwise', 'gamma': 0.1, 'eta': 0.2, 'colsample_by_level': 1.0}\n",
      "Iteration 2 - Best hyperparameters: {'subsample': 0.7000000000000001, 'random_state': 24, 'objective': 'reg:squarederror', 'n_jobs': -1, 'n_estimators': 50, 'min_child_weight': 5, 'max_depth': 4, 'lambda': 9, 'grow_policy': 'depthwise', 'gamma': 0.2, 'eta': 0.1, 'colsample_by_level': 0.8}\n",
      "Iteration 3 - Best hyperparameters: {'subsample': 0.6000000000000001, 'random_state': 24, 'objective': 'binary:logistic', 'n_jobs': -1, 'n_estimators': 50, 'min_child_weight': 1, 'max_depth': 10, 'lambda': 8, 'grow_policy': 'lossguide', 'gamma': 0.1, 'eta': 0.2, 'colsample_by_level': 1.0}\n",
      "Iteration 4 - Best hyperparameters: {'subsample': 0.9, 'random_state': 24, 'objective': 'reg:logistic', 'n_jobs': -1, 'n_estimators': 50, 'min_child_weight': 5, 'max_depth': 4, 'lambda': 4, 'grow_policy': 'depthwise', 'gamma': 0.2, 'eta': 0.1, 'colsample_by_level': 0.8}\n",
      "Iteration 5 - Best hyperparameters: {'subsample': 0.2, 'random_state': 24, 'objective': 'reg:logistic', 'n_jobs': -1, 'n_estimators': 50, 'min_child_weight': 1, 'max_depth': 4, 'lambda': 9, 'grow_policy': 'lossguide', 'gamma': 0.1, 'eta': 0.2, 'colsample_by_level': 0.8}\n",
      "Iteration 6 - Best hyperparameters: {'subsample': 0.7000000000000001, 'random_state': 24, 'objective': 'reg:logistic', 'n_jobs': -1, 'n_estimators': 50, 'min_child_weight': 1, 'max_depth': 23, 'lambda': 4, 'grow_policy': 'lossguide', 'gamma': 0.1, 'eta': 0.30000000000000004, 'colsample_by_level': 1.0}\n",
      "Iteration 7 - Best hyperparameters: {'subsample': 0.2, 'random_state': 24, 'objective': 'binary:logistic', 'n_jobs': -1, 'n_estimators': 50, 'min_child_weight': 1, 'max_depth': 16, 'lambda': 1, 'grow_policy': 'lossguide', 'gamma': 0.1, 'eta': 0.1, 'colsample_by_level': 0.8}\n",
      "Iteration 8 - Best hyperparameters: {'subsample': 0.2, 'random_state': 24, 'objective': 'reg:squarederror', 'n_jobs': -1, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 13, 'lambda': 2, 'grow_policy': 'depthwise', 'gamma': 0.2, 'eta': 0.1, 'colsample_by_level': 1.0}\n",
      "Iteration 9 - Best hyperparameters: {'subsample': 0.7000000000000001, 'random_state': 24, 'objective': 'reg:logistic', 'n_jobs': -1, 'n_estimators': 50, 'min_child_weight': 1, 'max_depth': 23, 'lambda': 4, 'grow_policy': 'lossguide', 'gamma': 0.1, 'eta': 0.30000000000000004, 'colsample_by_level': 1.0}\n",
      "Iteration 10 - Best hyperparameters: {'subsample': 0.2, 'random_state': 24, 'objective': 'binary:logistic', 'n_jobs': -1, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 9, 'lambda': 8, 'grow_policy': 'depthwise', 'gamma': 0, 'eta': 0.1, 'colsample_by_level': 0.8}\n",
      "Best hyperparameters: {'subsample': 0.2, 'random_state': 24, 'objective': 'reg:squarederror', 'n_jobs': -1, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 13, 'lambda': 2, 'grow_policy': 'depthwise', 'gamma': 0.2, 'eta': 0.1, 'colsample_by_level': 1.0}\n",
      "Accuracy score for Train: 0.9771986970684039\n",
      "Accuracy score for Test: 0.7207792207792207\n",
      "Train/Test difference: 0.25641947628918316\n"
     ]
    }
   ],
   "source": [
    "# Set a random seed for reproducibility\n",
    "random.seed(24)\n",
    "\n",
    "# Define hyperparameters for XGBoost\n",
    "hyperparameters_x = {\n",
    "    \"n_estimators\": [50, 100, 150, 200],\n",
    "    \"objective\": [\"reg:squarederror\", \"reg:logistic\", \"binary:logistic\"],\n",
    "    \"subsample\": np.arange(0, 1, 0.1),\n",
    "    \"max_depth\": [None] + list(np.arange(1, 25)),\n",
    "    \"gamma\": [0, 0.1, 0.2],\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "    \"colsample_by_level\": [0.8, 1.0],\n",
    "    \"grow_policy\": [\"depthwise\", \"lossguide\"],\n",
    "    \"n_jobs\": [-1],\n",
    "    \"lambda\": np.arange(1, 10),\n",
    "    \"eta\": np.arange(0, 1, 0.1),\n",
    "    \"random_state\": [24]\n",
    "}\n",
    "\n",
    "# Perform random search with different random states\n",
    "boost_model = XGBClassifier(random_state=24)\n",
    "num_iterations = 10\n",
    "results = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    random_state = np.random.randint(1, 100)\n",
    "    random_search = RandomizedSearchCV(boost_model, hyperparameters_x, n_iter=100, cv=5, scoring=\"accuracy\", random_state=random_state)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_hyper = random_search.best_params_\n",
    "    print(f\"Iteration {i + 1} - Best hyperparameters: {best_hyper}\")\n",
    "\n",
    "    results.append((best_hyper, random_search.best_score_))\n",
    "\n",
    "# Select the best set of hyperparameters based on the average cross-validation performance\n",
    "best_hyper, _ = max(results, key=lambda x: x[1])\n",
    "print(f\"Best hyperparameters: {best_hyper}\")\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "final_model = XGBClassifier(**best_hyper)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred_train = final_model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Accuracy score for Train: {train_accuracy}\")\n",
    "\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Accuracy score for Test: {test_accuracy}\")\n",
    "\n",
    "# Calculate and display the difference between the training and test sets\n",
    "difference = train_accuracy - test_accuracy\n",
    "print(f\"Train/Test difference: {difference}\")\n",
    "\n",
    "# Save the optimized model to a file\n",
    "dump(final_model, open(\"/workspaces/alfonsoMG_boosting/models/x_boosting_optimized_model.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After implementing both boosting models (XGBoost Classifier and Gradient Boost Classifier), several conclusions can be drawn. Firstly, it is evident that the Gradient Boosting model from sklearn exhibits the least overfitting, despite having the lowest accuracy. The difference between the training and test accuracies is minimal in this case. Although XGBoosting achieves the highest accuracy, its significant overfitting discourages its selection in favor of the other model. Upon optimization, the accuracy of XGBoosting substantially increases, but the disparity with the test accuracy is further exacerbated, leading to greater overfitting.\n",
    "\n",
    "For these reasons, the decision has been made to retain the Gradient Boosting model as the most effective option. While it may not present optimal conditions, it demonstrates the least amount of overfitting compared to the alternatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
